{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli process is a single trial that ends with either success (that occurs with p probability) or failure. If you want to know:\n",
    " * How many successes in n trials? -> Binomial\n",
    " * Will this single trial be a success? -> Bernoulli\n",
    " * How many trials until the 1st success? -> Geometric\n",
    " * How many trials until the r-th success? -> Negative binomial\n",
    " * Given M successes out of N trials, how many of the first n are successes? -> Hypergeometric\n",
    "\n",
    "I think of the Poisson process as the continuous version of the Bernoulli. Time is discrete in Bernoulli trials vs continuous for Poisson. If you want to know:\n",
    " * How many events occur in an interval of length (t)? -> Poisson\n",
    " * How long until your first event? -> Exponential\n",
    " * How long until the r-th event? -> Gamma\n",
    " * Given (A+B) events in a given interval, what fraction of the interval will it take until the Ath event occurs? -> Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution Help\n",
    "http://www.math.wm.edu/~leemis/chart/UDR/UDR.html\n",
    "\n",
    "http://www.cs.elte.hu/~mesti/valszam/kepletek.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "diag = [ mat[i][i] for i in range(len(mat)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag\n",
    "\n",
    "np.random.choice() #allows you to set probability weights for simulating probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Linear Regression With Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "#to split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit your model using the training set\n",
    "linear = LinearRegression()\n",
    "model = linear.fit(X_train, y_train)\n",
    "model.summary()  #LOTS OF INFO!!!\n",
    "\n",
    "# Call predict to get the predicted values for training and test set\n",
    "train_predicted = linear.predict(X_train)\n",
    "test_predicted = linear.predict(X_test)\n",
    "\n",
    "#k-fold cross validation implementation\n",
    "sklearn.cross_validation.cross_val_score()\n",
    "cv_shuffle = cross_validation.ShuffleSplit(train_size=train_size, test_size=200, n=len(y), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Assumptions\n",
    "Linear relationship. Multivariate normality. No or little multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.newaxis(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simulating\n",
    "scs.distribution.rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                               n_clusters_per_class=2, n_samples=1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "tpr, fpr, thresholds = roc_curve(probabilities, y_test)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\")\n",
    "plt.title(\"ROC plot of fake data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools import add_constant\n",
    "\n",
    "X = df[['gre', 'gpa', 'rank']].values\n",
    "X_const = add_constant(X, prepend=True)\n",
    "y = df['admit'].values\n",
    "\n",
    "logit_model = Logit(y, X_const).fit()\n",
    "\n",
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use KFold cross validation to calculate average accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "kfold = KFold(len(y))\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for train_index, test_index in kfold:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X[train_index], y[train_index])\n",
    "    y_predict = model.predict(X[test_index])\n",
    "    y_true = y[test_index]\n",
    "    accuracies.append(accuracy_score(y_true, y_predict))\n",
    "    precisions.append(precision_score(y_true, y_predict))\n",
    "    recalls.append(recall_score(y_true, y_predict))\n",
    "\n",
    "print \"accuracy:\", np.average(accuracies)\n",
    "print \"precision:\", np.average(precisions)\n",
    "print \"recall:\", np.average(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Bias related to test error.\n",
    "\n",
    "The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs. Variance related to training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Distribution\n",
    "\n",
    "The beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://www.gaussianwaves.com/gaussianwaves/wp-content/uploads/2013/10/Bayes_theorem_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Info Gain = Impurity(parent) - sum(Impurity(children)* probability per child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gini Impurity: probability of still guessing incorrectly while still knowing \n",
    "\n",
    "Impurity for single node = 1 - (sum over all classes( probability_per_class^2))\n",
    "= sum over all classes(p_per_class * (1-p_per_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic Regression\n",
    "* Maximize the log likelihood\n",
    "* Accuracy:\n",
    "    * How often is our classifier correct?\n",
    "    * Tp + Tn / N + P\n",
    "* True Positive Rate- Sensitvity/Recall\n",
    "    * When a yes, how often do we predict yes?\n",
    "    * Tp / P\n",
    "* True Negative Rate\n",
    "    * Negatives correctly predicted as negative\n",
    "    * Tn / N\n",
    "* False Positive rate\n",
    "    * Negatives not correctly predicted as positive\n",
    "    * When no, how often do we predict no?\n",
    "    * Fp / N\n",
    "* False Negative Rate - Specificity\n",
    "    * Fn / P\n",
    "* Precision: Tp/(Fp + Tp)\n",
    "\n",
    "* False Positive: Type 1 error\n",
    "* False Negative: Type 2 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cost function\n",
    "![alt text](https://github.com/zipfian/DSI_Lectures/raw/master/gradient-descent/giovanna_thron/images/log_likelihood_gradient2.png)\n",
    "\n",
    "\n",
    "![alt text](https://github.com/zipfian/DSI_Lectures/raw/master/gradient-descent/giovanna_thron/images/logit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN\n",
    "* Find distance between k number of nearest neighbor to decide class\n",
    "* Pros\n",
    "    * Easy to train (save the data)\n",
    "    * Works with any number of classes\n",
    "    * Easy to add new training datapoints\n",
    "* Cons\n",
    "    * Very slow to predict, have to calculate distance to every point in data set\n",
    "    * Curse of dimensionality\n",
    "        * As dimensionality increases, kNN performance decreases; nearest neighbors no longer nearby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "* Calculate information gain to determine best split\n",
    "* Categorical\n",
    "    * Use Gini Impurity\n",
    "    * Majority Class at each leaf\n",
    "* Continuous\n",
    "    * Use Entropy\n",
    "    * Take average value of result at each leaf for expected value\n",
    "![alt text](http://image.slidesharecdn.com/untitled-150504104846-conversion-gate01/95/a-taste-of-random-decision-forests-on-apache-spark-21-638.jpg?cb=1430736669)\n",
    "\n",
    "* Decision Trees are high variance, prone to overfitting, can ease by pruning\n",
    "    * Control by\n",
    "        * Leaf size: stop when there's few data points at a node\n",
    "        * Depth: stop when a tree gets too deep\n",
    "        * Class mix: stop when some percent of data points are the same class\n",
    "        * error reduction: stop when info gains are too little\n",
    "* Pros\n",
    "    * Interpetable, feature interactions, cheap to predict, can model complex/non-linear, can handle missing values, mixed data, extensible\n",
    "* Cons\n",
    "    * Computationally greedy (local optima), expensive to train, super high variance\n",
    "\n",
    "#### Regression Decision Trees\n",
    "* Responses are real values, can't use cross-entropy or Gini Index\n",
    "* Choose best splits using RSS against mean of each leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "### Bias Variance Tradeoff\n",
    "#### Bias\n",
    "* Can our model represent the true best predictor?\n",
    "* Caused by choosing a function that is too simple or choosing a regularized parameter that is too strong\n",
    "#### Variance\n",
    "* Random noise due to the specifics of our training data\n",
    "* Gets better as we get more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Profit Curve\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
