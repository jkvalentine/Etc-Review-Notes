{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "* Not at all like bagging or randfom forest!\n",
    "* Idea: combine a set of 'weak' learners to form strong learner\n",
    "    * weak in that error rate is only slightly better than random guessing\n",
    "* How? Sequentially apply weak classification algorithm to modified versions of the data -> sequence of weak classifiers\n",
    "    * Each tree is grown using information from last tree\n",
    "\n",
    "## AdaBoost\n",
    "* Each tree is expert on attacking errors of predecessor\n",
    "* Iteratively re-weights observations based on errors\n",
    "\n",
    "## Gradient Boosted Regression Trees\n",
    "* Instead of fitting reweighted training observations, fit residuals to those of previous tree\n",
    "\n",
    "## Boosting for Regression Trees\n",
    "* set fhat equal to 0 so that the residual conveniently becomes y\n",
    "* for B trees\n",
    "    * fit a tree with d splits to the training data\n",
    "    * update fhat by adding in a shrunken version of the new tree (&#955; * fb)\n",
    "    * Update the residuals ri -> ri - (&#955; * fb)\n",
    "* Output the boosted model\n",
    "\n",
    "## Boosting\n",
    "* A single boosted tree has high bias, low variance\n",
    "* Many, many boosted trees have low bias, high variance\n",
    "* Potential for overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBRT in sklearn\n",
    "* Tree structure\n",
    "    * max_depth \n",
    "        * controls degree of interactions\n",
    "        * eg: latitutde and longitude\n",
    "        * not often larger than 4 or 6\n",
    "    * min_samples_per_leaf\n",
    "        * may not want terminal nodes with too few leaves\n",
    "* Shrinkage\n",
    "    * n_estimators\n",
    "        * number of trees grown\n",
    "    * learning_rate\n",
    "        * lower learning rate requires higher n_estimators\n",
    "* Stochastic Gradient Boosting\n",
    "    * max_features\n",
    "       * random subsample of features\n",
    "       * especially good when there are lots of features\n",
    "       * start with sqrt(num of features)\n",
    "    * sub_sample\n",
    "        * random subset of the training set\n",
    "    * can reduce runtime AND improve accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "* Discrete AdaBoost\n",
    "    * One of the most popular boosting algorithms\n",
    "    * Y:{-1,1} (no/False=-1, yes/True=1)\n",
    "    * err = 1/N * Sum from i=1 to N  of I(yi != G(xi))\n",
    "    * I= Indicator function\n",
    "    * Weighting incorrect observations more heavily to transfer this 'error knowledge' to the next tree\n",
    "    * our alpha weights give higher influence to more accurate classifiers, these are for any given tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging/RF/Boosting\n",
    "### Variable Importance\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
