{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Margin Classification\n",
    "* Maximize the distance between classes of data\n",
    "* Define a plane between groups of data with a normal vector to the plan. Take the dot product of the normal vector and an unknown point\n",
    "* Margin is the distance between our hyper plane and the closest points in the dataset\n",
    "* Points closest to hyperplane in data set will have dot product equal to 1\n",
    "* Training error cannot exist for this model to function\n",
    "* Find magnitude of w to maximize our width, same as minimizing (1/2) * w^2\n",
    "* Scale the features when running SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slack Veriables\n",
    "* Introduce slack variables!\n",
    "    * They penalize samples that are either within the margin, or evem on the wrong side of the boundary\n",
    "* Now our classifier has a soft margin, and it makes the tradeoff between maximixing correct classification and making the widest possible margin\n",
    "* C cannot be zero or negative; C is like 'care': if C is large, our model cares a lot about the width of the street, if it is small, our model doesn't care \n",
    "\n",
    "## Kernals\n",
    "* Allow us to get the dot product of vectors after being transformed to a higher dimensional space without having to know what that transformation is!\n",
    "* How complicated of a decision boundary can we draw? Inclusion of a kernal increases variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
