{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Y = f(x) + &#949;\n",
    "\n",
    "&#253; = fhat(x)\n",
    "\n",
    "Error = F(Y,Yhat)\n",
    "\n",
    "Adding an interaction term will **ALWAYS** lower your error\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "learned the sampling error in our data, we have learned the signal **AND** the noise\n",
    "\n",
    "we can: reduce our features\n",
    "\n",
    "### Underfiting\n",
    "\n",
    "Fail to properly learn the functional relationship in our data, we have not fully accounted for the signal\n",
    "\n",
    "we can: increase our features\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "bias in this context: Measure of rigidity of a model. Low bias model: prediction is unlikely to change given new sample data.\n",
    "\n",
    "variance in this context: Measure of flexibility of a model. High variance model: prediction likely to change given new sample data.\n",
    "\n",
    "A test set must only ever be used once to reduce the bias in our model. \n",
    "\n",
    "## K-fold Cross Validation\n",
    "\n",
    "Non-overlapping segmented data sets used to train and test data.\n",
    "\n",
    "A decision making aid \n",
    "\n",
    "## Subset selection\n",
    "\n",
    "iterate through features, pick the best model we find\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Normalize data by subracting the mean and dividing by the standard deviation\n",
    "\n",
    "Forces parameters to be small; Ridge is computationally easier because it is differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regrassion\n",
    "### Linear Regression W/ LASSO (L1) Regularization\n",
    "\n",
    "Coefficients never gonna be negative\n",
    "\n",
    "1. Tends to set coefficients exactly equal to zero\n",
    "* leads to 'sparse' models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn\n",
    "\n",
    "Regularization parameter they call &#955;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* have data\n",
    "* make models\n",
    "* train models on training data\n",
    "* cross validate models to see which one is the best\n",
    "    * compare mean squred errors\n",
    "* take the best one, get rid of the rest\n",
    "* model the entire train data on the best model choice\n",
    "* pass in test data to get our prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ridge(alpha=lambda^2)\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score[X.test, Y.Test]\n",
    "\n",
    "for i, (train, val) in enumberate(kfold):\n",
    "    X_val, Y_val = X_train[val], Y_train[val]\n",
    "    X_fold, Y_fold = X_train[train], Y_train[train]\n",
    "    model = Ridge[alpha = lambda[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
