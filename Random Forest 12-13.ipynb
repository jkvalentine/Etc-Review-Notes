{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "* More predictors can make a better model than any one predictor by itself\n",
    "\n",
    "* Average or weighted average\n",
    "\n",
    "### Simple Ensembles\n",
    "* Committees\n",
    "    * Regression: unweighted average\n",
    "    * Classification: majority vote\n",
    "* Wieghted Averages\n",
    "    * Give more weight to 'better' predictors\n",
    "* Predictor of Predictors\n",
    "    * Treat individual predictors as features in a different ensemble model\n",
    "    * MLXtend Python Package    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Pruning\n",
    "* Not available in Python- code it yourself!\n",
    "\n",
    "## Pre-Pruning\n",
    "* Limit depth of decision tree with max_depth, min_samples_lead, min_samples_split to limit overfitting\n",
    "\n",
    "## Boostrapping\n",
    "* Randomly draw a sample from a set of data with replacement\n",
    "* Generate a dataset the same size as the original\n",
    "\n",
    "## Overfitting\n",
    "* 'Memorizing' the training set\n",
    "* How to recognize overfitting- low training error, high test error, cross validation \n",
    "* Can prevent with cross validation, simplicity of model\n",
    "\n",
    "## Bias Variance Tradeoff\n",
    "### Bias\n",
    "* Can our model represent the true best predictor?\n",
    "* Caused by choosing a function that is too simple or choosing a regularized parameter that is too strong\n",
    "### Variance\n",
    "* Random noise due to the specifics of our training data\n",
    "* Gets better as we get more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "### Bootstrap Aggregating\n",
    "* Draw a random bootstrap sample of size n\n",
    "* Grow a decision tree from bootstrap sample\n",
    "    * At each node, split the node using the feature that provides the best split for maximum info gain\n",
    "* Repeat the bootstrap-decision tree steps k times\n",
    "* Aggregate the prediction by each tree to assign the class label by majority vote (or average for regression)\n",
    "* We're searching for our optimal point in the tradeoff between bias and variance\n",
    "### Random Forest\n",
    "* Do all the BAGGING steps plus:\n",
    "    * At each node ***RANDOMLY*** select d features without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Bag Error\n",
    "* Matches up with cross-validation checks pretty well\n",
    "* We calculate this for each decision tree after building the tree and running the out-of-bag data through it but before creating our next decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many trees in my forest?\n",
    "* Variance decreases with more treas\n",
    "* Run time scales linearly with more trees\n",
    "* #Trees = n_estimators in sklearn\n",
    "* More is still better, but wait until the end to run 50k trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "* Relative importance of the variables in my data\n",
    "* How do de find out which are the most important?\n",
    "### 1st Feature Importance\n",
    "* Find the information gain for each split, see how many data points go through each split, multiply information gain by # of data points that pass through it, find importance for each feature (add importances up for different splits if a feature goes through multiple splits)\n",
    "### 2nd Feature Importance\n",
    "* Features that have the greatest increase in OOB error when they are permutes are the most important. If OOB error doesn't change, it's not important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Pros\n",
    "* For out of the box model, very good accuracy\n",
    "* Can be very fast, trees trained in parallel\n",
    "* OOB estimates allow for an estimate of generalization error without needing CV\n",
    "* Can handle thousands of features and be used for feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
